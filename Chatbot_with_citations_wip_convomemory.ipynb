{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoXqA1F4A8yrXkgaOLHAaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "759cb59aa7314a5985dc20062c80f837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ae553d959b341cdb7a1aaa9b89d0721",
              "IPY_MODEL_da483af4d28c4ef2af6cd899dab36baf",
              "IPY_MODEL_ca43beb21f9f493e89fd9d9350c0204a"
            ],
            "layout": "IPY_MODEL_a2b7f20baa014cc5ad30dc12f1111268"
          }
        },
        "afec53283bc745e5b8ef049ef009b538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ebc25302d964caa8400b57073b71fef",
              "IPY_MODEL_14674d4a50ce4a4181205149db726f3e",
              "IPY_MODEL_b3da9355e2ce40469588460fb2a3dcf3"
            ],
            "layout": "IPY_MODEL_16f2d308c29e4577b53374702b9050e5"
          }
        },
        "c134bca13f8e470e91ab25a24a147bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_36d13b5b0800407280a0371c794e54f6",
              "IPY_MODEL_a518b9e7f20446f4a46304cebe553456",
              "IPY_MODEL_edc587e4005b4ed2a3ed0aa959d46743"
            ],
            "layout": "IPY_MODEL_09c23470664b48839fa141e4ceb11e8e"
          }
        },
        "feb41ddf51a346de961f039ebaa78e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddd52ca43230438c8ce509ebf8cc6e39",
              "IPY_MODEL_ebdf943835ef47179d569ca6e1b09d39",
              "IPY_MODEL_43efc606bf3f4182b11b093d9fc1d0c7"
            ],
            "layout": "IPY_MODEL_a0719a30c9454bc58e1b97f3f0997163"
          }
        },
        "651c1b93156f4c9aaf4595878fb0a0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e79a4ab1fce144a78ec8f21da0a8e0a3",
              "IPY_MODEL_2333e168e9a5472489700bac72a5520a",
              "IPY_MODEL_617fc6ce8ae3457bbc00af00dede21e4"
            ],
            "layout": "IPY_MODEL_0574dc9c1ef94298a83175c0dc0121db"
          }
        },
        "fcaf340210b644dfa08394b6d3377f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_666718c088d0465e8adb88f0664a12f3",
              "IPY_MODEL_53ca61272d7c444a9a49f3c25be8736c",
              "IPY_MODEL_801f11813882488cafd5fad6e0a0868a"
            ],
            "layout": "IPY_MODEL_4657c2ce5fc14d64b4763ede3422e813"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/timbosssds/ML/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "# !pip install -qU transformers accelerate einops langchain xformers bitsandbytes faiss-gpu sentence_transformers\n",
        "# !pip install pypdf\n",
        "# !pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XecVz8WVmcvN",
        "outputId": "9a415ac0-bbad-4fea-8f94-f66644c7ab6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.17.0\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MileStone 1\n",
        "# -- end-to-end (No Citations)\n",
        "\n",
        "# -- 1.0 Load the PDF document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "#PDF_PATH = open(r'/content/cba_fsg.pdf', mode='rb')\n",
        "PDF_PATH = '/content/cba_fsg.pdf'\n",
        "loader = PyPDF2.PdfReader(PDF_PATH)\n",
        "\n",
        "# -- 2.0 Create an empty string to store the text\n",
        "text = \"\"\n",
        "# Loop through each page of the PDF\n",
        "for page_num in range(len(loader.pages)):\n",
        "    # Get the page object\n",
        "    page = loader.pages[page_num]\n",
        "    # Extract the texst from the page and add it to the text variable\n",
        "    text += page.extract_text()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# -- 3.0 Process text\n",
        "def convert_list_to_string_with_speech_marks(list_items):\n",
        "  \"\"\"Converts a list to a string with speech marks at each end.\"\"\"\n",
        "  string = \" \".join(list_items)\n",
        "  string = f\"\\\"{string}\\\"\"\n",
        "  return string\n",
        "list_items = texts\n",
        "text = convert_list_to_string_with_speech_marks(list_items)\n",
        "\n",
        "# -- Model\n",
        "context=text\n",
        "from transformers import pipeline\n",
        "def generate_answer(question, context):\n",
        "  \"\"\"Generates an answer to a question given a context.\"\"\"\n",
        "  question_answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "  response = question_answerer(question=question, context=text, max_answer_len=1024, top_p=1)\n",
        "  response = response[list(response)[-1]]\n",
        "  return response\n",
        "\n",
        "# -- Call model and produce answer\n",
        "# Generate an answer to the question\n",
        "#question = \"what is the Eligibility criteria for the Product\"\n",
        "question = \"what is the date\"\n",
        "\n",
        "answer = generate_answer(question, context)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "759cb59aa7314a5985dc20062c80f837",
            "afec53283bc745e5b8ef049ef009b538",
            "c134bca13f8e470e91ab25a24a147bb8",
            "feb41ddf51a346de961f039ebaa78e06",
            "651c1b93156f4c9aaf4595878fb0a0c7",
            "fcaf340210b644dfa08394b6d3377f8e"
          ]
        },
        "id": "EsfhAbQI5bn-",
        "outputId": "cb2956af-9f1a-40ef-9d6d-4f6cc5d9e3b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "759cb59aa7314a5985dc20062c80f837"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afec53283bc745e5b8ef049ef009b538"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c134bca13f8e470e91ab25a24a147bb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "feb41ddf51a346de961f039ebaa78e06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "651c1b93156f4c9aaf4595878fb0a0c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcaf340210b644dfa08394b6d3377f8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 October 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDY6-Qga_Jk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MileStone 2\n",
        "# -- end-to-end (With Citations)\n",
        "# -- 1.0 Load the PDF document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "#PDF_PATH = open(r'/content/cba_fsg.pdf', mode='rb')\n",
        "PDF_PATH = '/content/cba_fsg.pdf'\n",
        "loader = PyPDF2.PdfReader(PDF_PATH)\n",
        "\n",
        "# -- 2.0 Create an empty string to store the text\n",
        "text = \"\"\n",
        "# Loop through each page of the PDF\n",
        "for page_num in range(len(loader.pages)):\n",
        "    # Get the page object\n",
        "    page = loader.pages[page_num]\n",
        "    # Extract the texst from the page and add it to the text variable\n",
        "    text += page.extract_text()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# -- 3.0 Process text\n",
        "def convert_list_to_string_with_speech_marks(list_items):\n",
        "  \"\"\"Converts a list to a string with speech marks at each end.\"\"\"\n",
        "  string = \" \".join(list_items)\n",
        "  string = f\"\\\"{string}\\\"\"\n",
        "  return string\n",
        "list_items = texts\n",
        "text = convert_list_to_string_with_speech_marks(list_items)\n",
        "\n",
        "# -- Model\n",
        "context=text\n",
        "# Define a function to generate an answer to a question with citations\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_answer_with_citations(question, context):\n",
        "    \"\"\"Generates an answer to a question given a context and provides citations.\"\"\"\n",
        "    question_answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "    response = question_answerer(question=question, context=context, max_answer_len=1024, top_p=1)\n",
        "    answer_text = response[list(response)[-1]]\n",
        "    answer_start = response[\"start\"]\n",
        "    answer_end = response[\"end\"]\n",
        "\n",
        "    # Get the passage containing the answer\n",
        "    start_index = max(0, context.rfind(\".\", 0, answer_start))\n",
        "    end_index = min(len(context), context.find(\".\", answer_end))\n",
        "    passage_with_answer = context[start_index:end_index].strip()\n",
        "\n",
        "    # Get the citation (page number) of the passage\n",
        "    page_number = context.count(\".\", 0, start_index) + 1\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer_text,\n",
        "        \"passage\": passage_with_answer,\n",
        "        \"citation\": f\"Page {page_number}\"\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "#question = \"what is the date\"\n",
        "question = \"what commision do you receive\"\n",
        "answer_info = generate_answer_with_citations(question, context)\n",
        "print(\"Answer:\", answer_info[\"answer\"])\n",
        "print(\"Passage:\", answer_info[\"passage\"])\n",
        "print(\"Citation:\", answer_info[\"citation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Qrubmmw_Jem",
        "outputId": "afe869b9-602f-4a86-ad51-23a795f945fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: via email, by telephone or fax\n",
            "Passage: . \n",
            "However, there are special arrangements in place for some \n",
            "products and services where we can receive your instructions \n",
            "electronically via email, by telephone or fax\n",
            "Citation: Page 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-A0cTHPa5bfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9ZWy1sOItkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xsV8zxyXItfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- All below is WIP..."
      ],
      "metadata": {
        "id": "0YEMKfx95bci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4knbIR2IuHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h65QqB14IuB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WIP - trying to get conversation history\n",
        "# -- end-to-end (With Citations)\n",
        "# -- 1.0 Load the PDF document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "#PDF_PATH = open(r'/content/cba_fsg.pdf', mode='rb')\n",
        "PDF_PATH = '/content/cba_fsg.pdf'\n",
        "loader = PyPDF2.PdfReader(PDF_PATH)\n",
        "\n",
        "# -- 2.0 Create an empty string to store the text\n",
        "text = \"\"\n",
        "# Loop through each page of the PDF\n",
        "for page_num in range(len(loader.pages)):\n",
        "  # Get the page object\n",
        "  page = loader.pages[page_num]\n",
        "  # Extract the texst from the page and add it to the text variable\n",
        "  text += page.extract_text()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# -- 3.0 Process text\n",
        "def convert_list_to_string_with_speech_marks(list_items):\n",
        "  \"\"\"Converts a list to a string with speech marks at each end.\"\"\"\n",
        "  string = \" \".join(list_items)\n",
        "  string = f\"\\\"{string}\\\"\"\n",
        "  return string\n",
        "  list_items = texts\n",
        "  text = convert_list_to_string_with_speech_marks(list_items)\n",
        "\n",
        "# -- 4.0 Maintain conversation history\n",
        "conversation_history = []\n",
        "\n",
        "# -- Model\n",
        "context=text\n",
        "\n",
        "# Define a function to generate an answer to a question with citations\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_answer_with_citations(question, context):\n",
        "  \"\"\"Generates an answer to a question given a context and provides citations.\"\"\"\n",
        "  # Check if the question is related to any of the previous questions\n",
        "  for previous_question, previous_answer in conversation_history:\n",
        "    if question.startswith(previous_question):\n",
        "      context = f\"{context} {previous_answer}\"\n",
        "\n",
        "  question_answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "  response = question_answerer(question=question, context=context, max_answer_len=1024, top_p=1)\n",
        "  answer_text = response[list(response)[-1]]\n",
        "  answer_start = response[\"start\"]\n",
        "  answer_end = response[\"end\"]\n",
        "\n",
        "  # Get the passage containing the answer\n",
        "  start_index = max(0, context.rfind(\".\", 0, answer_start))\n",
        "  end_index = min(len(context), context.find(\".\", answer_end))\n",
        "  passage_with_answer = context[start_index:end_index].strip()\n",
        "\n",
        "  # Get the citation (page number) of the passage\n",
        "  page_number = context.count(\".\", 0, start_index) + 1\n",
        "\n",
        "  # Add the new question and answer to the conversation history\n",
        "  conversation_history.append((question, answer_text))\n",
        "\n",
        "  return {\n",
        "      \"answer\": answer_text,\n",
        "      \"passage\": passage_with_answer,\n",
        "      \"citation\": f\"Page {page_number}\"}\n",
        "\n",
        "# Example usage\n",
        "#question = \"what is the date\" # original question\n",
        "\n",
        "#question = \"what is afca?\" # initial q to test follow up question\n",
        "question = \"what number do i contact you on?\" # follow up question to see if conversation memory working\n",
        "\n",
        "#question = \"what email do i use for dispute resolution?\" # Intial question\n",
        "#question = \"and phone number?\" # Intial question # Follow up\n",
        "\n",
        "answer_info = generate_answer_with_citations(question, context)\n",
        "print(\"Answer:\", answer_info[\"answer\"])\n",
        "print(\"Passage:\", answer_info[\"passage\"])\n",
        "print(\"Citation:\", answer_info[\"citation\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBT83oF8AF5A",
        "outputId": "3aae8991-17a1-43fd-fdb0-1dc05d5276f9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1300 555 727\n",
            "Passage: .au/feedback\n",
            "National Relay Service \n",
            "• TTY or Voice: Call 133 677  then as k for 13 2221\n",
            "• Speak & listen: Call 1300 555 727  then as k for 13 2221\n",
            "• SMS relay – Text  0423 677 767\n",
            "For more information, visit the National Relay Service: \n",
            "relayservice\n",
            "Citation: Page 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td2XSwCfAFym",
        "outputId": "aba434f4-ea0b-4562-fee3-2ac81825dfbb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('what number do i contact you on?', '1300 555 727')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rehELAQ-AFvg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- WIP (trying to use langchain memory)\n",
        "# MileStone 2\n",
        "# -- end-to-end (With Citations)\n",
        "# -- 1.0 Load the PDF document\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import PyPDF2\n",
        "#PDF_PATH = open(r'/content/cba_fsg.pdf', mode='rb')\n",
        "PDF_PATH = '/content/cba_fsg.pdf'\n",
        "loader = PyPDF2.PdfReader(PDF_PATH)\n",
        "\n",
        "\n",
        "# -- New\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "\n",
        "\n",
        "# -- 2.0 Create an empty string to store the text\n",
        "text = \"\"\n",
        "# Loop through each page of the PDF\n",
        "for page_num in range(len(loader.pages)):\n",
        "    # Get the page object\n",
        "    page = loader.pages[page_num]\n",
        "    # Extract the texst from the page and add it to the text variable\n",
        "    text += page.extract_text()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_text(text)\n",
        "\n",
        "# -- 3.0 Process text\n",
        "def convert_list_to_string_with_speech_marks(list_items):\n",
        "  \"\"\"Converts a list to a string with speech marks at each end.\"\"\"\n",
        "  string = \" \".join(list_items)\n",
        "  string = f\"\\\"{string}\\\"\"\n",
        "  return string\n",
        "list_items = texts\n",
        "text = convert_list_to_string_with_speech_marks(list_items)\n",
        "\n",
        "# -- Model\n",
        "context=text\n",
        "# Define a function to generate an answer to a question with citations\n",
        "from transformers import pipeline\n",
        "\n",
        "def generate_answer_with_citations(question, context, memory):\n",
        "    \"\"\"Generates an answer to a question given a context and provides citations.\"\"\"\n",
        "    question_answerer = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
        "    response = question_answerer(question=question, context=context, max_answer_len=1024, top_p=1)\n",
        "    answer_text = response[list(response)[-1]]\n",
        "    answer_start = response[\"start\"]\n",
        "    answer_end = response[\"end\"]\n",
        "\n",
        "    # -- New\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\")\n",
        "\n",
        "    # Get the passage containing the answer\n",
        "    start_index = max(0, context.rfind(\".\", 0, answer_start))\n",
        "    end_index = min(len(context), context.find(\".\", answer_end))\n",
        "    passage_with_answer = context[start_index:end_index].strip()\n",
        "\n",
        "    # Get the citation (page number) of the passage\n",
        "    page_number = context.count(\".\", 0, start_index) + 1\n",
        "\n",
        "    return {\n",
        "        \"answer\": answer_text,\n",
        "        \"passage\": passage_with_answer,\n",
        "        \"citation\": f\"Page {page_number}\"\n",
        "    }\n",
        "\n",
        "# -- New\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# Example usage\n",
        "question = \"what number do i contact you on?\" # follow up question to see if conversation memory working\n",
        "\n",
        "#question = \"what email do i use for dispute resolution?\" # Intial question\n",
        "#question = \"and phone number?\" # Intial question # Follow up\n",
        "answer_info = generate_answer_with_citations(question, context, memory)\n",
        "print(\"Answer:\", answer_info[\"answer\"])\n",
        "print(\"Passage:\", answer_info[\"passage\"])\n",
        "print(\"Citation:\", answer_info[\"citation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELVIvz5M6Kw9",
        "outputId": "39c784b7-41f7-40dd-eaf6-5de173982789"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: \n",
            "FSG\n",
            "Passage: .  You may request further details of these programmes \n",
            "by contacting us within a reasonable time after receiving this \n",
            "FSG and before any financial service is provided to you\n",
            "Citation: Page 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WdE7SbWD6Ktt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujMgBwhl2Oc_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
